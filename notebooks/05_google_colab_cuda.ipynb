{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab_intro"
   },
   "source": [
    "# TensorLogic on Google Colab with CUDA GPU\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mathews-Tom/TensorLogic/blob/main/notebooks/05_google_colab_cuda.ipynb)\n",
    "\n",
    "This notebook demonstrates TensorLogic's CUDA backend on Google Colab's T4 GPU.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Google Colab with GPU runtime (Runtime > Change runtime type > T4 GPU)\n",
    "- The notebook will automatically install all dependencies\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Setup TensorLogic on Google Colab\n",
    "2. Using the CUDA backend with CuPy\n",
    "3. GPU-accelerated logical reasoning\n",
    "4. Temperature-controlled inference on GPU\n",
    "5. Performance comparison: CUDA vs CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## Step 1: Verify GPU and Install Dependencies\n",
    "\n",
    "First, let's verify we have GPU access and install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install TensorLogic from GitHub\n",
    "!pip install git+https://github.com/Mathews-Tom/TensorLogic.git\n",
    "\n",
    "# Install CuPy for CUDA 11.x (Colab's CUDA version)\n",
    "!pip install cupy-cuda11x scipy\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Installation complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_cupy"
   },
   "outputs": [],
   "source": [
    "# Verify CuPy installation and CUDA access\n",
    "import cupy as cp\n",
    "\n",
    "# Get device info\n",
    "device = cp.cuda.Device()\n",
    "props = cp.cuda.runtime.getDeviceProperties(device.id)\n",
    "mem_info = device.mem_info\n",
    "\n",
    "print(\"CUDA Device Information:\")\n",
    "print(f\"  Device Name: {props['name'].decode()}\")\n",
    "print(f\"  Compute Capability: {device.compute_capability}\")\n",
    "print(f\"  Total Memory: {mem_info[1] / (1024**3):.2f} GB\")\n",
    "print(f\"  Free Memory: {mem_info[0] / (1024**3):.2f} GB\")\n",
    "print(f\"\\nCuPy Version: {cp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports_header"
   },
   "source": [
    "## Step 2: Import TensorLogic\n",
    "\n",
    "Now let's import TensorLogic and create a CUDA backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Core TensorLogic imports\n",
    "from tensorlogic import (\n",
    "    # High-level API\n",
    "    quantify,\n",
    "    reason,\n",
    "    # Backend\n",
    "    create_backend,\n",
    "    # Logical operations\n",
    "    logical_and,\n",
    "    logical_or,\n",
    "    logical_not,\n",
    "    logical_implies,\n",
    "    # Quantifiers\n",
    "    exists,\n",
    "    forall,\n",
    ")\n",
    "from tensorlogic.compilation import create_strategy\n",
    "\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "print(\"TensorLogic imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_backend"
   },
   "outputs": [],
   "source": [
    "# Create CUDA backend explicitly\n",
    "cuda_backend = create_backend(\"cuda\")\n",
    "print(f\"CUDA Backend: {type(cuda_backend).__name__}\")\n",
    "\n",
    "# Also create NumPy backend for comparison\n",
    "numpy_backend = create_backend(\"numpy\")\n",
    "print(f\"NumPy Backend: {type(numpy_backend).__name__}\")\n",
    "\n",
    "# Test auto-detection (should pick CUDA on Colab)\n",
    "auto_backend = create_backend(\"auto\")\n",
    "print(f\"Auto-detected Backend: {type(auto_backend).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuda_ops_header"
   },
   "source": [
    "## Step 3: GPU-Accelerated Logical Operations\n",
    "\n",
    "Let's test the core logical operations on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuda_basic_ops"
   },
   "outputs": [],
   "source": [
    "# Create tensors on GPU using CuPy\n",
    "a = cp.array([1.0, 1.0, 0.0, 0.0])  # True, True, False, False\n",
    "b = cp.array([1.0, 0.0, 1.0, 0.0])  # True, False, True, False\n",
    "\n",
    "print(\"Input tensors (on GPU):\")\n",
    "print(f\"  a = {cp.asnumpy(a)}\")\n",
    "print(f\"  b = {cp.asnumpy(b)}\")\n",
    "print(f\"  Device: {a.device}\")\n",
    "\n",
    "# Logical operations\n",
    "print(\"\\nLogical Operations:\")\n",
    "print(f\"  a AND b = {cp.asnumpy(logical_and(a, b, backend=cuda_backend))}\")\n",
    "print(f\"  a OR b  = {cp.asnumpy(logical_or(a, b, backend=cuda_backend))}\")\n",
    "print(f\"  NOT a   = {cp.asnumpy(logical_not(a, backend=cuda_backend))}\")\n",
    "print(f\"  a -> b  = {cp.asnumpy(logical_implies(a, b, backend=cuda_backend))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knowledge_graph_header"
   },
   "source": [
    "## Step 4: Knowledge Graph on GPU\n",
    "\n",
    "Let's create a larger knowledge graph and run queries on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knowledge_graph"
   },
   "outputs": [],
   "source": [
    "# Define a family knowledge graph\n",
    "entities = [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Henry\"]\n",
    "n = len(entities)\n",
    "entity_to_idx = {name: i for i, name in enumerate(entities)}\n",
    "\n",
    "# Create Parent relation on GPU\n",
    "parent_cpu = np.zeros((n, n), dtype=np.float32)\n",
    "\n",
    "# Family tree:\n",
    "# Alice & Bob -> Carol, David\n",
    "# Carol & Eve -> Frank\n",
    "# David & Grace -> Henry\n",
    "parent_facts = [\n",
    "    (\"Alice\", \"Carol\"), (\"Alice\", \"David\"),\n",
    "    (\"Bob\", \"Carol\"), (\"Bob\", \"David\"),\n",
    "    (\"Carol\", \"Frank\"), (\"Eve\", \"Frank\"),\n",
    "    (\"David\", \"Henry\"), (\"Grace\", \"Henry\"),\n",
    "]\n",
    "\n",
    "for p, c in parent_facts:\n",
    "    parent_cpu[entity_to_idx[p], entity_to_idx[c]] = 1.0\n",
    "\n",
    "# Transfer to GPU\n",
    "parent = cp.asarray(parent_cpu)\n",
    "\n",
    "print(f\"Parent relation matrix shape: {parent.shape}\")\n",
    "print(f\"Data location: GPU (CuPy array)\")\n",
    "print(f\"\\nParent facts loaded: {len(parent_facts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grandparent_inference"
   },
   "outputs": [],
   "source": [
    "# Infer Grandparent relation: exists y: Parent(x,y) AND Parent(y,z)\n",
    "# Using einsum for relation composition\n",
    "composition = cuda_backend.einsum('xy,yz->xz', parent, parent)\n",
    "grandparent = cuda_backend.step(composition)\n",
    "\n",
    "print(\"Grandparent Inference (GPU-computed):\")\n",
    "grandparent_cpu = cp.asnumpy(grandparent)\n",
    "\n",
    "for i, name_i in enumerate(entities):\n",
    "    for j, name_j in enumerate(entities):\n",
    "        if grandparent_cpu[i, j] > 0:\n",
    "            print(f\"  {name_i} is grandparent of {name_j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quantifier_queries"
   },
   "outputs": [],
   "source": [
    "# Quantified queries on GPU\n",
    "print(\"Quantified Queries:\")\n",
    "\n",
    "# Who has children? (EXISTS)\n",
    "has_children = exists(parent, axis=1, backend=cuda_backend)\n",
    "print(\"\\nWho has children (EXISTS)?\")\n",
    "has_children_cpu = cp.asnumpy(has_children)\n",
    "for i, name in enumerate(entities):\n",
    "    if has_children_cpu[i] > 0:\n",
    "        print(f\"  {name}: Yes\")\n",
    "\n",
    "# Who are leaves (no children)? (NOT EXISTS = FORALL NOT)\n",
    "is_leaf = logical_not(has_children, backend=cuda_backend)\n",
    "print(\"\\nWho are leaf nodes (no children)?\")\n",
    "is_leaf_cpu = cp.asnumpy(is_leaf)\n",
    "for i, name in enumerate(entities):\n",
    "    if is_leaf_cpu[i] > 0:\n",
    "        print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "temperature_header"
   },
   "source": [
    "## Step 5: Temperature-Controlled Reasoning\n",
    "\n",
    "The breakthrough feature: controlling reasoning from pure deduction (T=0) to analogical inference (T>0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "temperature_demo"
   },
   "outputs": [],
   "source": [
    "# Create fuzzy/uncertain relations\n",
    "similarity = cp.asarray([\n",
    "    [1.0, 0.9, 0.3, 0.2],\n",
    "    [0.9, 1.0, 0.4, 0.3],\n",
    "    [0.3, 0.4, 1.0, 0.8],\n",
    "    [0.2, 0.3, 0.8, 1.0],\n",
    "], dtype=cp.float32)\n",
    "\n",
    "items = [\"Cat\", \"Dog\", \"Sparrow\", \"Robin\"]\n",
    "\n",
    "print(\"Similarity Matrix (fuzzy knowledge):\")\n",
    "print(\"         \", end=\"\")\n",
    "for item in items:\n",
    "    print(f\"{item:>8}\", end=\"\")\n",
    "print()\n",
    "for i, item in enumerate(items):\n",
    "    print(f\"{item:>8}\", end=\"\")\n",
    "    for j in range(len(items)):\n",
    "        print(f\"{cp.asnumpy(similarity[i, j]):>8.2f}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "temperature_comparison"
   },
   "outputs": [],
   "source": [
    "# Compare different temperatures\n",
    "print(\"Temperature-Controlled Inference:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "temperatures = [0.0, 0.2, 0.5, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    # Apply temperature scaling\n",
    "    if temp == 0.0:\n",
    "        # Hard threshold at T=0\n",
    "        scaled = cuda_backend.step(similarity - 0.5)\n",
    "        mode = \"Pure Deductive (exact)\"\n",
    "    else:\n",
    "        # Soft sigmoid-like scaling\n",
    "        scaled = 1.0 / (1.0 + cp.exp(-((similarity - 0.5) / temp)))\n",
    "        if temp <= 0.2:\n",
    "            mode = \"Cautious (minimal generalization)\"\n",
    "        elif temp <= 0.5:\n",
    "            mode = \"Balanced (moderate generalization)\"\n",
    "        else:\n",
    "            mode = \"Analogical (creative inference)\"\n",
    "    \n",
    "    print(f\"\\nTemperature T={temp}: {mode}\")\n",
    "    print(f\"  Cat-Dog similarity: {cp.asnumpy(scaled[0, 1]):.3f}\")\n",
    "    print(f\"  Cat-Sparrow similarity: {cp.asnumpy(scaled[0, 2]):.3f}\")\n",
    "    print(f\"  Sparrow-Robin similarity: {cp.asnumpy(scaled[2, 3]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compilation_header"
   },
   "source": [
    "## Step 6: Compilation Strategies\n",
    "\n",
    "TensorLogic supports multiple semantic interpretations. Let's test them on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compilation_strategies"
   },
   "outputs": [],
   "source": [
    "# Test all compilation strategies\n",
    "strategies = [\n",
    "    (\"soft_differentiable\", \"Neural network training with logic constraints\"),\n",
    "    (\"hard_boolean\", \"Exact verification, theorem proving\"),\n",
    "    (\"godel\", \"Similarity scoring, fuzzy matching\"),\n",
    "    (\"product\", \"Probabilistic inference\"),\n",
    "    (\"lukasiewicz\", \"Bounded multi-hop reasoning\"),\n",
    "]\n",
    "\n",
    "# Test values\n",
    "p = cp.asarray([0.8, 0.4, 0.9])\n",
    "q = cp.asarray([0.6, 0.7, 0.3])\n",
    "\n",
    "print(\"Compilation Strategy Comparison:\")\n",
    "print(f\"  P = {cp.asnumpy(p)}\")\n",
    "print(f\"  Q = {cp.asnumpy(q)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for strategy_name, description in strategies:\n",
    "    strategy = create_strategy(strategy_name)\n",
    "    \n",
    "    # Compute P AND Q\n",
    "    p_np, q_np = cp.asnumpy(p), cp.asnumpy(q)\n",
    "    result_and = strategy.logical_and(p_np, q_np)\n",
    "    result_or = strategy.logical_or(p_np, q_np)\n",
    "    result_implies = strategy.logical_implies(p_np, q_np)\n",
    "    \n",
    "    print(f\"\\n{strategy_name}:\")\n",
    "    print(f\"  Use case: {description}\")\n",
    "    print(f\"  P AND Q: {result_and}\")\n",
    "    print(f\"  P OR Q:  {result_or}\")\n",
    "    print(f\"  P -> Q:  {result_implies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "benchmark_header"
   },
   "source": [
    "## Step 7: Performance Benchmark - CUDA vs CPU\n",
    "\n",
    "Let's compare the performance of CUDA and NumPy backends on larger knowledge graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benchmark_setup"
   },
   "outputs": [],
   "source": [
    "def benchmark_knowledge_graph(n_entities, backend, backend_name, n_iterations=10):\n",
    "    \"\"\"Benchmark knowledge graph operations.\"\"\"\n",
    "    # Generate random sparse knowledge graph\n",
    "    np.random.seed(42)\n",
    "    density = 0.1  # 10% of edges exist\n",
    "    kg_np = (np.random.random((n_entities, n_entities)) < density).astype(np.float32)\n",
    "    \n",
    "    # Transfer to appropriate device\n",
    "    if backend_name == \"CUDA\":\n",
    "        kg = cp.asarray(kg_np)\n",
    "    else:\n",
    "        kg = kg_np\n",
    "    \n",
    "    # Warmup\n",
    "    _ = backend.einsum('xy,yz->xz', kg, kg)\n",
    "    if backend_name == \"CUDA\":\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "    \n",
    "    # Benchmark: Relation composition (grandparent-like inference)\n",
    "    start = time.time()\n",
    "    for _ in range(n_iterations):\n",
    "        composition = backend.einsum('xy,yz->xz', kg, kg)\n",
    "        result = backend.step(composition)\n",
    "        if backend_name == \"CUDA\":\n",
    "            cp.cuda.Stream.null.synchronize()\n",
    "    elapsed = (time.time() - start) / n_iterations\n",
    "    \n",
    "    return elapsed * 1000  # Return in milliseconds\n",
    "\n",
    "print(\"Benchmark Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_benchmark"
   },
   "outputs": [],
   "source": [
    "# Run benchmarks at different scales\n",
    "sizes = [100, 500, 1000, 2000, 5000]\n",
    "cuda_times = []\n",
    "numpy_times = []\n",
    "\n",
    "print(\"Performance Benchmark: CUDA vs NumPy\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Entities':>10} {'CUDA (ms)':>12} {'NumPy (ms)':>12} {'Speedup':>10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for n in sizes:\n",
    "    cuda_time = benchmark_knowledge_graph(n, cuda_backend, \"CUDA\")\n",
    "    numpy_time = benchmark_knowledge_graph(n, numpy_backend, \"NumPy\")\n",
    "    speedup = numpy_time / cuda_time\n",
    "    \n",
    "    cuda_times.append(cuda_time)\n",
    "    numpy_times.append(numpy_time)\n",
    "    \n",
    "    print(f\"{n:>10} {cuda_time:>12.2f} {numpy_time:>12.2f} {speedup:>9.1f}x\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCUDA provides {np.mean([n/c for c, n in zip(cuda_times, numpy_times)]):.1f}x average speedup!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benchmark_visualization"
   },
   "outputs": [],
   "source": [
    "# Visualize the benchmark results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Execution time comparison\n",
    "x = np.arange(len(sizes))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, numpy_times, width, label='NumPy (CPU)', color='#4285f4')\n",
    "ax1.bar(x + width/2, cuda_times, width, label='CUDA (GPU)', color='#34a853')\n",
    "ax1.set_xlabel('Knowledge Graph Size (entities)')\n",
    "ax1.set_ylabel('Execution Time (ms)')\n",
    "ax1.set_title('TensorLogic Performance: CPU vs GPU')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(sizes)\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Speedup\n",
    "speedups = [n/c for c, n in zip(cuda_times, numpy_times)]\n",
    "ax2.plot(sizes, speedups, 'o-', color='#ea4335', linewidth=2, markersize=8)\n",
    "ax2.fill_between(sizes, 1, speedups, alpha=0.3, color='#ea4335')\n",
    "ax2.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Knowledge Graph Size (entities)')\n",
    "ax2.set_ylabel('Speedup (x)')\n",
    "ax2.set_title('CUDA Speedup over NumPy')\n",
    "ax2.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tensorlogic_cuda_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBenchmark visualization saved to 'tensorlogic_cuda_benchmark.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "large_scale_header"
   },
   "source": [
    "## Step 8: Large-Scale Knowledge Graph Demo\n",
    "\n",
    "Let's demonstrate TensorLogic on a larger knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "large_scale_demo"
   },
   "outputs": [],
   "source": [
    "# Create a large knowledge graph (10,000 entities)\n",
    "N = 10000\n",
    "print(f\"Creating knowledge graph with {N:,} entities...\")\n",
    "\n",
    "# Generate sparse relations\n",
    "np.random.seed(42)\n",
    "density = 0.001  # Very sparse (0.1%)\n",
    "\n",
    "# Create on CPU, then transfer to GPU\n",
    "kg_np = (np.random.random((N, N)) < density).astype(np.float32)\n",
    "kg_gpu = cp.asarray(kg_np)\n",
    "\n",
    "n_edges = int(kg_np.sum())\n",
    "print(f\"Number of edges: {n_edges:,}\")\n",
    "print(f\"Edge density: {n_edges / (N*N) * 100:.3f}%\")\n",
    "print(f\"Memory on GPU: {kg_gpu.nbytes / (1024**2):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "large_scale_inference"
   },
   "outputs": [],
   "source": [
    "# Run multi-hop inference on the large graph\n",
    "print(f\"\\nRunning multi-hop inference on {N:,} entities...\")\n",
    "\n",
    "# 2-hop inference (like grandparent)\n",
    "start = time.time()\n",
    "hop2 = cuda_backend.einsum('xy,yz->xz', kg_gpu, kg_gpu)\n",
    "hop2_bool = cuda_backend.step(hop2)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "time_2hop = time.time() - start\n",
    "\n",
    "print(f\"2-hop inference completed in {time_2hop*1000:.1f} ms\")\n",
    "print(f\"  New inferred edges: {int(cp.sum(hop2_bool)):,}\")\n",
    "\n",
    "# 3-hop inference\n",
    "start = time.time()\n",
    "hop3 = cuda_backend.einsum('xy,yz->xz', hop2_bool, kg_gpu)\n",
    "hop3_bool = cuda_backend.step(hop3)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "time_3hop = time.time() - start\n",
    "\n",
    "print(f\"3-hop inference completed in {time_3hop*1000:.1f} ms\")\n",
    "print(f\"  New inferred edges: {int(cp.sum(hop3_bool)):,}\")\n",
    "\n",
    "print(f\"\\nTotal inference time: {(time_2hop + time_3hop)*1000:.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quantifier_large"
   },
   "outputs": [],
   "source": [
    "# Quantified queries on large graph\n",
    "print(\"\\nQuantified Queries on Large Graph:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# EXISTS: Find all nodes with outgoing edges\n",
    "start = time.time()\n",
    "has_outgoing = exists(kg_gpu, axis=1, backend=cuda_backend)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "time_exists = time.time() - start\n",
    "\n",
    "n_with_outgoing = int(cp.sum(has_outgoing > 0))\n",
    "print(f\"Nodes with outgoing edges: {n_with_outgoing:,} ({time_exists*1000:.2f} ms)\")\n",
    "\n",
    "# EXISTS: Find all nodes with incoming edges\n",
    "start = time.time()\n",
    "has_incoming = exists(kg_gpu, axis=0, backend=cuda_backend)\n",
    "cp.cuda.Stream.null.synchronize()\n",
    "time_exists2 = time.time() - start\n",
    "\n",
    "n_with_incoming = int(cp.sum(has_incoming > 0))\n",
    "print(f\"Nodes with incoming edges: {n_with_incoming:,} ({time_exists2*1000:.2f} ms)\")\n",
    "\n",
    "# Compute connectivity statistics\n",
    "out_degree = cp.sum(kg_gpu, axis=1)\n",
    "in_degree = cp.sum(kg_gpu, axis=0)\n",
    "\n",
    "print(f\"\\nConnectivity Statistics:\")\n",
    "print(f\"  Max out-degree: {int(cp.max(out_degree))}\")\n",
    "print(f\"  Max in-degree: {int(cp.max(in_degree))}\")\n",
    "print(f\"  Avg degree: {float(cp.mean(out_degree)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary_header"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you've learned:\n",
    "\n",
    "1. **CUDA Setup**: How to configure TensorLogic with CuPy on Google Colab\n",
    "2. **GPU Operations**: Running logical operations on NVIDIA T4 GPU\n",
    "3. **Knowledge Graphs**: Creating and querying relation tensors on GPU\n",
    "4. **Temperature Control**: The breakthrough feature for reasoning control\n",
    "5. **Compilation Strategies**: Different semantic interpretations\n",
    "6. **Performance**: CUDA provides significant speedups for larger graphs\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Temperature Dial**: T=0 for provable deduction, T>0 for analogical reasoning\n",
    "- **GPU Acceleration**: Essential for 1000+ entity knowledge graphs\n",
    "- **Unified Framework**: Same API works on MLX (Apple), CUDA (NVIDIA), and NumPy (CPU)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try with your own knowledge graph data\n",
    "- Experiment with different compilation strategies\n",
    "- Scale to even larger graphs (100K+ entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup"
   },
   "outputs": [],
   "source": [
    "# Cleanup GPU memory\n",
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "print(\"GPU memory cleaned up.\")\n",
    "print(\"\\nThank you for trying TensorLogic!\")\n",
    "print(\"GitHub: https://github.com/Mathews-Tom/TensorLogic\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
