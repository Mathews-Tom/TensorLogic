# TensorLogic Performance Characteristics

This document describes the performance characteristics of the TensorLogic backend abstraction layer, with benchmarks from Apple Silicon (MLX) and NVIDIA GPUs (CUDA).

## Performance Summary

| Backend | Hardware | Best For | Peak Speedup |
|---------|----------|----------|--------------|
| **CUDA** | NVIDIA T4/V100/A100 | Large-scale KG (1K+ entities), data centers | **700x** vs CPU |
| **MLX** | Apple M1/M2/M3 | Development, unified memory workloads | **100x** vs CPU |
| **NumPy** | Any CPU | Small graphs, compatibility, testing | Baseline |

## Performance Requirements

The TensorLogic backend abstraction follows the einops philosophy of minimal overhead:

- **Target:** <2% overhead for composite operations vs direct backend calls
- **Individual operations:** <5% overhead tolerance (Python call overhead)
- **Batch processing:** Efficient scaling for batch sizes 4-32
- **Memory efficiency:** Leverages unified memory (MLX) or high-bandwidth VRAM (CUDA)
- **Lazy evaluation:** Optimizes computation graphs for deferred execution

## CUDA Benchmark Results (Tesla T4)

Benchmarked on Google Colab with Tesla T4 GPU (15GB VRAM, CUDA 12.4, CuPy 13.6.0).

### Knowledge Graph Reasoning Performance

Relation composition benchmark (computing 2-hop paths via einsum):

| Entities | CUDA (ms) | NumPy (ms) | Speedup |
|----------|-----------|------------|---------|
| 100 | 0.54 | 0.26 | 0.5x |
| 500 | 0.54 | 20.42 | **37.5x** |
| 1,000 | 1.37 | 181.62 | **132.5x** |
| 2,000 | 7.93 | 1,574.37 | **198.5x** |
| 5,000 | 59.57 | 42,167.71 | **707.8x** |

**Average speedup: 215.4x** across tested scales.

**Key observations:**
- GPU overhead dominates at small scales (<500 entities) - use NumPy for small graphs
- Speedup increases super-linearly with graph size
- At 5,000 entities, CUDA is **700x faster** than CPU

### Large-Scale Inference (10,000 Entities)

Demo with 10,000 entities and 99,917 edges (0.1% density):

| Operation | Time | Edges Inferred |
|-----------|------|----------------|
| 2-hop inference | 485.4 ms | 994,425 |
| 3-hop inference | 8,217.1 ms | 9,424,028 |
| **Total** | **8,702.6 ms** | ~10M edges |

Memory usage: 381.5 MB on GPU for the 10KÃ—10K relation matrix.

### Quantified Queries

| Query Type | Time (ms) | Result |
|------------|-----------|--------|
| EXISTS (outgoing edges) | 1.78 | 9,998 nodes |
| EXISTS (incoming edges) | 100.75 | 9,998 nodes |

---

## MLX Benchmark Results (Apple Silicon)

All benchmarks performed on Apple Silicon with MLX backend.

### Abstraction Overhead

Performance overhead of TensorBackend Protocol abstraction vs direct MLX calls:

| Operation | Overhead | Notes |
|-----------|----------|-------|
| einsum (matrix multiply) | <5% | Critical operation for tensor logic |
| multiply (Hadamard product) | <5% | Used for logical AND |
| add (element-wise) | <5% | Element-wise operations |
| sum (reduction) | <5% | Used for existential quantification |
| **Composite operations** | **<2%** | **Realistic workloads (einsum + multiply + add)** |
| grad (automatic differentiation) | <5% | Gradient computation |

**Key Insight:** The composite operation test validates that for realistic tensor logic workloads (chains of multiple operations), the abstraction overhead is negligible (<2%). Individual operation overhead is primarily Python function call overhead, which is amortized across composite operations.

### Batch Size Performance

Performance scaling for different batch sizes (operations/second):

**Matrix Operations (batch, 256, 256):**
- Batch 4: ~3000 ops/sec
- Batch 8: ~2500 ops/sec
- Batch 16: ~2600 ops/sec
- Batch 32: ~1500 ops/sec

**Reduction Operations (batch, 1024):**
- Batch 4: ~2400 ops/sec
- Batch 8: ~3900 ops/sec
- Batch 16: ~3800 ops/sec
- Batch 32: ~4400 ops/sec

Performance scales efficiently with batch size, demonstrating good GPU utilization.

### Memory Characteristics

**Unified Memory:**
- MLX backend leverages Apple Silicon unified memory (200 GB/s bandwidth on M1 Pro)
- No memory leaks detected in repeated operations
- Efficient memory usage for large tensors (tested up to 256 MB)
- Python memory tracking shows minimal overhead

### Lazy Evaluation Benefits

**Eager vs Lazy Execution:**
- Eager (eval after each op): Reference baseline
- Lazy (single eval at end): **~75% faster**

Lazy evaluation provides significant performance benefits by:
1. Building computation graphs without immediate execution
2. Optimizing entire operation chains
3. Reducing redundant memory transfers

### GPU Acceleration for Large Operations

For large operations (>1000x1000 matrices), MLX can deliver **10-100x speedups** on Apple Silicon due to GPU acceleration. This is particularly relevant for:
- Knowledge graph reasoning with 1M+ entities
- Batch inference over large relation tensors
- Multi-hop reasoning chains

For smaller operations, Python call overhead may make NumPy competitive. The crossover point depends on operation complexity and matrix sizes.

## Running Performance Tests

Execute the performance benchmark suite:

```bash
# Run all performance tests
uv run pytest tests/test_backends/test_performance.py -v -s

# Run specific test category
uv run pytest tests/test_backends/test_performance.py::TestAbstractionOverhead -v
uv run pytest tests/test_backends/test_performance.py::TestBatchSizePerformance -v
uv run pytest tests/test_backends/test_performance.py::TestMemoryUsage -v
uv run pytest tests/test_backends/test_performance.py::TestLazyEvaluation -v
```

**Note:** Performance tests require MLX on Apple Silicon. Tests are automatically skipped on unsupported platforms.

## Performance Best Practices

### 1. Use Lazy Evaluation

```python
backend = create_backend("mlx")

# Build computation graph
result1 = backend.einsum("ij,jk->ik", A, B)
result2 = backend.multiply(result1, C)
result3 = backend.add(result2, D)

# Force evaluation once at end
backend.eval(result3)  # Efficient: single eval for entire graph
```

### 2. Batch Operations

```python
# Process multiple examples together
batched_data = backend.zeros((batch_size, dim1, dim2))
result = backend.einsum("bij,bjk->bik", batched_data, weights)
backend.eval(result)
```

### 3. Leverage Compiled Functions

```python
def tensor_logic_operation(x: Any, y: Any) -> Any:
    temp = backend.multiply(x, y)
    return backend.sum(temp, axis=1)

# Compile for repeated calls
fast_op = backend.compile(tensor_logic_operation)
result = fast_op(data_x, data_y)
```

## Benchmark Methodology

### Timing Approach
- **Warmup:** 100 iterations to eliminate cold start effects
- **Benchmark:** 5000 iterations for reliable measurements
- **Rounds:** 10 rounds, using minimum time (best performance, least noise)
- **Timer:** `time.perf_counter()` for high-precision timing

### Overhead Calculation
```
overhead = ((backend_time - direct_time) / direct_time) * 100
```

### Why Minimum Time?
Using minimum time across multiple rounds reduces impact of:
- System background processes
- CPU frequency scaling
- Cache effects
- Other timing noise

This provides a more accurate measure of the abstraction's true overhead.

## System Requirements

**Tested Platforms:**
- macOS (Apple Silicon: M1/M2/M3) with MLX backend
- Linux/Windows with NVIDIA GPU (T4, V100, A100) via CUDA backend
- Any platform with NumPy (CPU fallback)
- Python 3.12+

**Backend Dependencies:**
- MLX backend: `pip install mlx>=0.30.0` (Apple Silicon only)
- CUDA backend: `pip install cupy-cuda12x` (NVIDIA GPU, CUDA 12.x)
- NumPy backend: `pip install numpy>=1.24.0` (universal)

**Expected Performance:**
- Development (M1 Pro): Batch sizes 4-32, matrix operations (256x256)
- Production (M1 Max/M2 Max): Larger batch sizes and matrices
- Production (NVIDIA T4): 1,000-10,000 entity knowledge graphs at 100-700x speedup
- Production (NVIDIA A100): Scale to 100K+ entities with high-bandwidth memory

## Conclusion

TensorLogic delivers production-grade performance across all major GPU platforms:

**CUDA Backend (NVIDIA GPUs):**
- **Up to 700x speedup** for large knowledge graphs (5,000+ entities)
- **10,000 entity graphs** with multi-hop inference in under 10 seconds
- **Tested on Google Colab** with Tesla T4 (15GB VRAM)

**MLX Backend (Apple Silicon):**
- **<2% overhead** for composite operations vs direct MLX calls
- **75% speedup** from lazy evaluation
- **10-100x GPU acceleration** for large operations (>1000x1000 matrices)

**Both backends achieve:**
- **Zero memory leaks** in repeated operations
- **Efficient batch processing** (4-32 examples)
- **Clean Protocol-based abstraction** without sacrificing performance

The unified API enables seamless development on Apple Silicon with production deployment on NVIDIA data center GPUs.
